{"pages":[{"title":"DDPM: Denoising Diffusion Probabilistic Models","text":"概述","tags":"paper","url":"/ddpm-denoising-diffusion-probabilistic-models.html"},{"title":"NCSN: Generative Modeling by Estimating Gradients of the Data Distribution","text":"概述 NeurIPS 2019","tags":"paper","url":"/ncsn-generative-modeling-by-estimating-gradients-of-the-data-distribution.html"},{"title":"Score-Based Generative Modeling through Stochastic Differential Equations","text":"概述 Using SDE and reverse-time SDE to extend to continous domain. 使用SDE及反向SDE，将时间T 推广到 连续域 Generalize Score matching and Diffusion to inifinite T. 统一 score matching 和 扩散模型， 并将时间 T 推广到 无限 Two SDEs derived from 2 Markov chain to use 两个具体的SDE， 分别来自不同的Markov chain, 其中一个是 DDPM使用的chain. Train like Denoising Score Matching or DDPM. Sliced Score matching also works. 训练方式与 denoising score matching/DDPM 很相似 Inference/Generation: 1. General SDE sampler using the corresponding reverse-time SDE. 2. Propose Predictor-Corrector sampler (SDE Solver + MCMC) 生成过程 1. 使用常用的SDE数值解法， 2. 提出 用 score-based MCMC 改进 Propose a Deterministic sampler, call \"probability flow ordinary DE (ODE)\". 提出确定性采样器来生成， 好处有多 Architecture Improvements 结构改进 class-conditional generation, image imputation and colorization， 实现可控制生成， 基于又一个SDE。 Unified Framework The authors proposed a unified framework generalizes score matching NCSN and DDPM It uses Stochastic Differential Equation(SDE) SDE 中文 and reverse-time SDE ( derivation English ) to extend discrete T (>1000) to infinite continuous T. The general form of SDE is: $d\\boldsymbol{x} = f(\\boldsymbol{x}, t) dt \\;\\;\\;\\; + G(\\boldsymbol{x}, t) d\\boldsymbol{w}$ Compared to the diffusion: $x_t \\sim \\mathcal{N}(\\sqrt{1-\\beta_t} x_{t-1}, \\;\\; \\beta_t \\boldsymbol{I})$ Two detailed SDEs for the framework Variance Exploding (VE) SDE: $dx = \\sqrt{\\frac{d [\\sigma&#94;2(t)]}{dt}}dw$ , derived from the Markov Chain: $x_i = x_{i-1} + \\sqrt{\\sigma_i&#94;2 - \\sigma_{i-1}&#94;2} z_{i-1}$, where $z_{i-1} \\sim \\mathcal{N}(0, \\boldsymbol{I})$ . Variance Preserving (VP) SDE: $dx = -1/2 \\beta(t) x dt + \\sqrt{\\beta(t)} dw$, derived from DDPM's discrete Markov chain. Training Train a time-dependent score-based model Eq 7, similar to denoising score matching and also DDPM Inference/Generation/Sampling after training Apply general SDE Sampler Propose Predictor-Corrector sampler (SDE Solver + MCMC) Deterministic sampler They proposed a Deterministic sampler, called \"probability flow ordinary DE (ODE)\" Advantages (v.s. Stochastic sample): Exact likelihood computation (DDPM has its own computation, iDDPM improves the results) Manipulating latent representations, for image editing, such as interpolation, and temperature scaling. Uniquely identifiable encoding Efficient sampling, reduce T>1000 -> T<100. Architecture of U-Net In Appendix H 5 different improvents And Exponential Moving Average Controllable Generation By solving a conditional reverse-time SDE. Tasks: class-conditional generation, image imputation and colorization","tags":"paper","url":"/score-based-generative-modeling-through-stochastic-differential-equations.html"},{"title":"Score Matching: Estimation of non-normalized statistical models by score matching","text":"概述 $p_{\\theta}(x) = \\frac{e&#94;{-E_\\theta(x)}}{Z(\\theta)}$, $Z(\\theta) = \\int_x e&#94;{-E_\\theta(x)}$配分函数 是对所有 x 的积分, 不可计算， 但是对x 来说是个常数。 所以 最大对数似然估计 MLE 对 $\\theta$求导更新 需要 MCMC采样， 很慢 但 $p_{\\theta}(x)$","tags":"paper","url":"/score-matching-estimation-of-non-normalized-statistical-models-by-score-matching.html"},{"title":"StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN","text":"概述 stylegan 改进 AdaIN, 移除 normalization, 但保持输入与输出方差不变。 不再生成带气泡点的或异常的图片 添加正则化， 避免 隐空间 w 区域的拉伸或挤压， 任意方向梯度相等 发现 PPL perceptual path length 与 图片质量 相关 改进结构， 判别器使用 残差， 生成器只使用 skip 跳链， 保持 Progressive Growing 可视化分析对分辨率resolution的利用程度， 发现使用更大模型可以提高 对更高分辨率的利用 正则项使得生成器 $x=g(w)$ 求逆结果更准确更唯一， $x=g(w), w'=g&#94;{-1}(x), x'=g(w'), x\\approx x'$ 2 归一化改进 原版: 第i层AdaIN $AdaIN(x, y_i) = y_{s,i} \\frac{x - \\mu(x)}{\\sigma(x)} + y_{b, i}$ , 其中 $y_i = (y_{s, i}, y_{b, i}) = A_i (w), w = 8MLP(z)$ 缺点： 生成 blob有气泡部位 或 corrupted异常 的图片 改进一 移除 AdaIN里的平移项bias $y_b$， 即只保留 scaling项 $y_s$ 作用类似 std 改进二 Demodulation Demodulation 移除 normalization操作， 转成 w -> w' = s w -> w'' = w'/ $\\sqrt{\\sum w'}$ 3 基于PPL的正则化 3.1 感知质量评估 PPL(perceptual path length)跟感知图像质量的关系, PPL 越小， 图像质量更好 4 改进 progressive growing 4.1","tags":"paper","url":"/stylegan2-analyzing-and-improving-the-image-quality-of-stylegan.html"},{"title":"StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks","text":"概述 bilinear up/down sampling, 更长的训练， 调参 应用 AdaIN 到 Progressive GAN w = 8层MLP(z), 再用 A(w)->scale,bias of AdaIN 移除过去的 z输入给conv， 变成 固定的 Noise 作为 stochastic variantion 控制细节变化 mixing regularization. 部分数据使用 style mixing生成的图片","tags":"paper","url":"/stylegan-a-style-based-generator-architecture-for-generative-adversarial-networks.html"},{"title":"JEM: Your Classifier is secretely an Energy Based Model","text":"概述 已知 多类别分类任务概率写作 Usually, for a multi-class task, the probability $p(y|x)$ is written as $$ p(y|x) = \\frac{exp(f_{\\theta}(x)[y])}{\\sum_{y'}exp(f_{\\theta}(x)[y'])} $$ 其中 $f_{\\theta}(x)[y]$ 为 logits输出, where $f_{\\theta}(x)[y]$ is the logits output. 作者假设 Following the definition of EBM, define the joint density $p(x, y)$ as: $$ p(x, y) = \\frac{exp(f_{\\theta}(x)[y])}{Z(\\theta)} $$ Hence, it's easily to derive $$ p(x) = \\sum_{y} p(x, y) = \\frac{\\sum_{y} exp(f_{\\theta}(x)[y])}{Z(\\theta)} $$ They derive a new energy function in the multi-class task. 可得到一个能量函数 $E_\\theta(x) = -\\log \\sum_{y} exp(f_{\\theta}(x)[y])$ 及对应的能量模型 因此， 优化目标函数为 Combine the energy-based model with the classifier, the objective function is $$ \\log p_\\theta(x, y) = \\log p_\\theta(x) + \\log p_\\theta(y|x) $$","tags":"paper","url":"/jem-your-classifier-is-secretely-an-energy-based-model.html"}]}