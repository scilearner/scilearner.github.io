{"pages":[{"title":"DDPM: Denoising Diffusion Probabilistic Models","text":"概述","tags":"paper","url":"/ddpm-denoising-diffusion-probabilistic-models.html"},{"title":"NCSN: Generative Modeling by Estimating Gradients of the Data Distribution","text":"概述 NeurIPS 2019","tags":"paper","url":"/ncsn-generative-modeling-by-estimating-gradients-of-the-data-distribution.html"},{"title":"Score-Based Generative Modeling through Stochastic Differential Equations","text":"概述 Using SDE and reverse-time SDE in continous domain. Generalize Score matching and Diffusion to inifinite T. Two SDEs to use Train like Denoising Score Matching or DDPM. Sliced Score matching also works. Inference/Generation: Propose Predictor-Corrector sampler (SDE Solver + MCMC) Propose a Deterministic sampler, call \"probability flow ordinary DE (ODE)\". Architecture Improvements class-conditional generation, image imputation and colorization Unified Framework The authors proposed a unified framework generalizes score matching NCSN and DDPM It uses Stochastic Differential Equation(SDE) SDE 中文 and reverse-time SDE ( derivation English ) to extend discrete T (>1000) to infinite continuous T. The general form of SDE is: $d\\boldsymbol{x} = f(\\boldsymbol{x}, t) dt \\;\\;\\;\\; + G(\\boldsymbol{x}, t) d\\boldsymbol{w}$ Compared to the diffusion: $x_t \\sim \\mathcal{N}(\\sqrt{1-\\beta_t} x_{t-1}, \\;\\; \\beta_t \\boldsymbol{I})$ Two detailed SDEs for the framework Variance Exploding (VE) SDE: $dx = \\sqrt{\\frac{d [\\sigma&#94;2(t)]}{dt}}dw$ , derived from the Markov Chain: $x_i = x_{i-1} + \\sqrt{\\sigma_i&#94;2 - \\sigma_{i-1}&#94;2} z_{i-1}$, where $z_{i-1} \\sim \\mathcal{N}(0, \\boldsymbol{I})$ . Variance Preserving (VP) SDE: $dx = -1/2 \\beta(t) x dt + \\sqrt{\\beta(t)} dw$, derived from DDPM's discrete Markov chain. Training Train a time-dependent score-based model Eq 7, similar to denoising score matching and also DDPM Inference/Generation/Sampling after training Apply general SDE Sampler Propose Predictor-Corrector sampler (SDE Solver + MCMC) Deterministic sampler They proposed a Deterministic sampler, called \"probability flow ordinary DE (ODE)\" Advantages (v.s. Stochastic sample): Exact likelihood computation (DDPM has its own computation, iDDPM improves the results) Manipulating latent representations, for image editing, such as interpolation, and temperature scaling. Uniquely identifiable encoding Efficient sampling, reduce T>1000 -> T<100. Architecture of U-Net In Appendix H 5 different improvents And Exponential Moving Average Controllable Generation By solving a conditional reverse-time SDE. Tasks: class-conditional generation, image imputation and colorization","tags":"paper","url":"/score-based-generative-modeling-through-stochastic-differential-equations.html"},{"title":"Score Matching: Estimation of non-normalized statistical models by score matching","text":"概述 $p_{\\theta}(x) = \\frac{e&#94;{-E_\\theta(x)}}{Z(\\theta)}$, $Z(\\theta) = \\int_x e&#94;{-E_\\theta(x)}$配分函数 是对所有 x 的积分, 不可计算， 但是对x 来说是个常数。 所以 最大对数似然估计 MLE 对 $\\theta$求导更新 需要 MCMC采样， 很慢 但 $p_{\\theta}(x)$","tags":"paper","url":"/score-matching-estimation-of-non-normalized-statistical-models-by-score-matching.html"},{"title":"StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN","text":"概述 stylegan 改进 AdaIN 造成的气泡点 归一化 原版: 第i层AdaIN $AdaIN(x, y_i) = y_{s,i} \\frac{x - \\mu(x)}{\\sigma(x)} + y_{b, i}$ , 其中 $y_i = (y_{s, i}, y_{b, i}) = A_i (w), w = 8MLP(z)$ 缺点： 生成 blob有气泡部位 或 corrupted异常 的图片 改进一 移除 AdaIN里的平移项bias $y_b$， 即只保留 scaling项 $y_s$ 作用类似 std 改进二 Demodulation Demodulation 移除 normalization操作， 转成 w -> w' = s w -> w'' = w'/ $\\sqrt{\\sum w'}$ 正则化 PPL(perceptual path length)跟感知图像质量的关系","tags":"paper","url":"/stylegan2-analyzing-and-improving-the-image-quality-of-stylegan.html"},{"title":"StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks","text":"概述 bilinear up/down sampling, 更长的训练， 调参 应用 AdaIN 到 Progressive GAN w = 8层MLP(z), 再用 A(w)->scale,bias of AdaIN 移除过去的 z输入给conv， 变成 固定的 Noise 作为 stochastic variantion 控制细节变化 mixing regularization. 部分数据使用 style mixing生成的图片","tags":"paper","url":"/stylegan-a-style-based-generator-architecture-for-generative-adversarial-networks.html"},{"title":"JEM: Your Classifier is secretely an Energy Based Model","text":"概述 已知 多类别分类任务概率写作 $$ p(y|x) = \\frac{exp(f_{\\theta}(x)[y])}{\\sum_{y'}exp(f_{\\theta}(x)[y'])} $$ 其中 $f_{\\theta}(x)[y]$ 为 logits输出 作者假设 $$ p(x, y) = \\frac{exp(f_{\\theta}(x)[y])}{Z(\\theta)} \\ p(x) = \\sum_{y} p(x, y) = \\frac{\\sum_{y} exp(f_{\\theta}(x)[y])}{Z(\\theta)} \\ $$ 可得到一个能量函数 $E_\\theta(x) = -\\log \\sum_{y} exp(f_{\\theta}(x)[y])$ 及对应的能量模型 因此， 优化目标函数为 $$ \\log p_\\theta(x, y) = \\log p_\\theta(x) + \\log p_\\theta(y|x) $$","tags":"paper","url":"/jem-your-classifier-is-secretely-an-energy-based-model.html"}]}